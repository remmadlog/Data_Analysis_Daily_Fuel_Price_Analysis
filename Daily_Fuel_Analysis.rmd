---
title: "Daily German Fuel Price Analysis (2024)"
output: rmarkdown::github_document
---

# TOC
<details>
  <summary>Click to show</summary>

- [TOC](#toc)
- [Dataset](#dataset)
- [When Should You Refuel?](#when-should-you-refuel)
  * [Preparation and Transformation](#preparation-and-transformation)
  * [Analysis](#analysis)
    + [Fuel Price by Weekday](#fuel-price-by-weekday)
    + [Best Time of Day to Refuel](#best-time-of-day-to-refuel)
    + [Best Time to Refuel](#best-time-to-refuel)
    + [Best combination](#best-combination)
- [Where to Refuel?](#where-to-refuel)
  * [Brand Analysis](#brand-analysis)
- [Post Code Consideration](#post-code-consideration)
  * [Transforming and Loading the Data](#transforming-and-loading-the-data)
  * [Where and When to Refuel?](#where-and-when-to-refuel)
    + [Overview for e10](#overview-for-e10)
      - [Station per Day](#station-per-day)
      - [Station per Time of Day](#station-per-time-of-day)
      - [Station per Time of Day](#station-per-time-of-day-1)


</details>

---

# Dataset
The dataset we are using can be found [here](https://dev.azure.com/tankerkoenig/_git/tankerkoenig-data?path=/README.md&_a=preview).
There one can obtain way more data for different years.
We will only focus on the year 2024.

This dataset contains of several ``.csv`` files.
One file per day, sorted in folders by month.
One file is of the following form:

> The head contains:
>
> `date,station_uuid,diesel,e5,e10,dieselchange,e5change,e10change`
>
> Meaning:
>
> |Feld        | Bedeutung                               |
> |------------|-----------------------------------------|
> |date        | Time of change                          |
> |station_uuid| UUID of stations                        |
> |diesel      | Price Diesel                            |
> |e5          | Price Super E5                          |
> |e10         | Price Super E10                         |
> |dieselchange| 0=no change, 1=change, 2=removed, 3=new |
> |e5change    | 0=no change, 1=change, 2=removed, 3=new |
> |e10change   | 0=no change, 1=change, 2=removed, 3=new |


Since the dataset ist quit detailed, and therefor very large (about 11.5GB), we will focus on less information.
We will add a ``date_app`` column that rounds to the full hour and aggregate all by that and brand.
Therefore, we will no longer have the unique station information, but we will end up with a more manageable amount of data.

# When Should You Refuel?

## Preparation and Transformation

For each month there is a different folder labeled ``01`` to ``12``.
We will do the following:
- iterate over each month ``i in 1:12``
  - get a list of files (one file per day) per folder (one folder per month)
  - open each file (day)
    - join the station information, to obtain the ``brand`` column
    - create a date approximation column ``date_app``, contain date of day and the full hour
    - group by ``date_app`` and ``brand``

The script can be found [here](transforming_cleaning_agg_date_brand.R)
```r
# joining/merging dataframes
library(dplyr)
# general handling analytics
library(tidyverse)


# open station file
df_station <- read.csv("Datasets/stations.csv")

# initiate an empty dataframe to build upon
df <-  data.frame()

# getting a list of all files per month
# iterating over month
for (i in 1:12){
  # turn the int 3 into the str "03"
  if (i<10){
    folder <- paste0("0", toString(i))
  } else{
    folder <- toString(i)
  }
  path <- paste0("Datasets/2024/", folder)
  list_of_files <- list.files(path =path,
                            pattern = "\\.csv$",
                            full.names = TRUE)

  # iterating over each file (day) for the current month
  for (file_path in list_of_files){
    # open file (daily information)
    df_file <- read.csv(file_path) %>%
            left_join(df_station, by =c("station_uuid"="uuid")) %>%
            select(date, station_uuid, brand, diesel,e5,e10) %>%
            filter(diesel>0.7,
                    diesel<3,
                    e5>0.7,
                    e5<3,
                    e10>0.7,
                    e10<3)   # We do that to avoid foulty outleyers

    # looking at the ``brand`` we notice some whitspace, let's remove it
    df_file$brand <- str_squish(df_file$brand)

    # set date_app YYYY-MM-DD HH
    df_file$date_app <- str_split_i(df_file$date, ":",1)
    temp <- df_file %>%
            group_by(date_app, brand) %>%
            summarise(diesel = mean(diesel),
                      e5 = mean(e5),
                      e10 = mean(e10),
                      gr_size = n())

    # add the day of the week as column
    temp$weekday <- weekdays(as.Date(str_split_i(temp$date_app, " ",1)))

    # add hour column
    temp$hour <- str_split_i(temp$date_app, " ",2)

    # add time of day colums that provides the information of time of day by
    # # night, morning, midday, evening in even 6h chuncks
    # # tod - time of day
    temp$tod <-  NA
    temp$tod[temp$hour %in% c("00", "01", "02", "03", "04", "05")] <- "night"
    temp$tod[temp$hour %in% c("06", "07", "08", "09", "10", "11")] <- "morning"
    temp$tod[temp$hour %in% c("12", "13", "14", "15", "16", "17")] <- "midday"
    temp$tod[temp$hour %in% c("18", "19", "20", "21", "22", "23")] <- "evening"

    # combine df with temp -- add rows from temp
    df <- rbind(df, temp)

    # remove ``temp`` and ``df_file`` from memory
    rm(temp,df_file)
  }
}

write.csv(df,file='Datasets/agg_dataset.csv', row.names=FALSE)
```


## Analysis
With the creation of our dataframe behind us, we will now process to do some analysis.
To be more precise we will have a look at the following:
- price per fuel per
  - ``hour``
  - ``weekday``
  - ``tod`` (time of day)
- price per fuel per ``brand`` per
  - ``hour``
  - ``weekday``
  - ``tod`` (time of day)
- price per fuel per brand over time

The main question ``When and where to refuel?`` we be answered during this investigation.

We start by loading the ``tidyverse`` library and reading the aggregated CSV file.
```{r warning=FALSE,message=FALSE}
# general handling analytics
library(tidyverse)

# for .md tables
library(pander)
panderOptions('table.split.table', 'inf')

# open station file
df <- read.csv("Datasets/agg_dataset.csv")
```

### Fuel Price by Weekday
We group by ``weekday`` and consider the average fuel price.
```{r warning=FALSE,message=FALSE}
df %>%
    group_by(weekday) %>%
    summarise(diesel=mean(diesel),
              e5=mean(e5),
              e10=mean(e10))
```
```{r}
pander(head(df,10), style = 'rmarkdown')
```

We notice that there is not a huge differance in pricing.
This might be due to larger differences in prices per month.

To see if that might be the case, we first add the column ``month``
```{r}
df$month <- str_split_i(df$date_app, "-",2)
```

Then we take the average:
```{r warning=FALSE,message=FALSE}
df %>%
        group_by(month) %>%
    summarise(diesel=mean(diesel),
              e5=mean(e5),
              e10=mean(e10))
```
```{r}
pander(head(df,10), style = 'rmarkdown')
```

So averaging over, e.g., every **monday**, results in an outcome that's to flat.

We will try to count the weekdays that are lowest per week.
For that we need to know the numer of the week:
```{r}
df$nweek <- strftime(str_split_i(df$date_app, " ",1), format = "%V")
```


Let us focus in **e10** for a moment.

Next we filter by ``nweek`` and ``week``, form the average and consider columns were the minimum for each group is present.
Then we group by ``weekday`` to count the entries.
We should end up with a list of weekdays and how often they were the lowest price
```{r warning=FALSE,message=FALSE}
df_weekdays <- df %>%
    group_by(nweek, weekday) %>%
    summarise(e10=mean(e10)) %>%
    filter(e10 == min(e10, na.rm=TRUE)) %>%
    group_by(weekday) %>%
    summarise(amount_e10 = n()) %>%
    arrange(desc(amount_e10))
```

For **e10** we see that **Mondays** and **Tuesdays** are the best ways to refile in general.

Doing the same for **diesel** and **e5**:
```{r warning=FALSE,message=FALSE}
temp <- df %>%
    group_by(nweek, weekday) %>%
    summarise(e5=mean(e5)) %>%
    filter(e5 == min(e5, na.rm=TRUE)) %>%
    group_by(weekday) %>%
    summarise(amount_e5 = n()) %>%
    arrange(desc(amount_e5))

df_weekdays <- left_join(df_weekdays,temp, by = "weekday")

temp <- df %>%
    group_by(nweek, weekday) %>%
    summarise(diesel=mean(diesel)) %>%
    filter(diesel == min(diesel, na.rm=TRUE)) %>%
    group_by(weekday) %>%
    summarise(amount_diesel = n()) %>%
    arrange(desc(amount_diesel))

df_weekdays <- left_join(df_weekdays,temp, by = "weekday")
```

Print the results:
```{r}
pander(df_weekdays, style = 'rmarkdown')
```


Using this procedure we continue to figure out the best time to refuel.
### Best Time of Day to Refuel

Adding a ``day`` column.
```{r warning=FALSE,message=FALSE}
df$day <- str_split_i(df$date_app, " ",1)
```

Then following the same idea as before.
Considering the following table,
```{r warning=FALSE,message=FALSE}
df_tod <- df %>%
    group_by(day, tod) %>%
    summarise(e10=mean(e10)) %>%
    filter(e10 == min(e10)) %>%
    group_by(tod) %>%
    summarise(amount_e10 = n()) %>%
    arrange(desc(amount_e10))

temp <- df %>%
    group_by(day, tod) %>%
    summarise(e5=mean(e5)) %>%
    filter(e5 == min(e5)) %>%
    group_by(tod) %>%
    summarise(amount_e5 = n()) %>%
    arrange(desc(amount_e5))

df_tod <- left_join(df_tod, temp, by="tod")


temp <- df %>%
    group_by(day, tod) %>%
    summarise(diesel=mean(diesel)) %>%
    filter(diesel == min(diesel)) %>%
    group_by(tod) %>%
    summarise(amount_diesel = n()) %>%
    arrange(desc(amount_diesel))

df_tod <- left_join(df_tod, temp, by="tod")
```

Printing the output
```{r}
pander(df_tod, style = 'rmarkdown')
```
We notice that the best time of day is **evening**.
Refueling in the evening is cheaper for 366 out of 366 days in 2024.


### Best Time to Refuel
We learned the day and the general time of day to refuel.
Next we will see at witch exact hour of the day one should refill.
```{r warning=FALSE,message=FALSE}
df_h <- df %>%
    group_by(day, hour) %>%
    summarise(e10=mean(e10)) %>%
    filter(e10 == min(e10)) %>%
    group_by(hour) %>%
    summarise(amount_e10 = n()) %>%
    arrange(desc(amount_e10))

temp <- df %>%
    group_by(day, hour) %>%
    summarise(e5=mean(e5)) %>%
    filter(e5 == min(e5)) %>%
    group_by(hour) %>%
    summarise(amount_e5 = n()) %>%
    arrange(desc(amount_e5))

df_h <- left_join(df_h, temp, by="hour")


temp <- df %>%
    group_by(day, hour) %>%
    summarise(diesel=mean(diesel)) %>%
    filter(diesel == min(diesel)) %>%
    group_by(hour) %>%
    summarise(amount_diesel = n()) %>%
    arrange(desc(amount_diesel))

df_h <- left_join(df_h, temp, by="hour")
```

Having a look at the table
```{r}
pander(df_h, style = 'rmarkdown')
```
we notice that best time to refuel is a 09:00pm.

### Best combination

We do the same as before but we group for all components, namely for ``nweek, weekday`` and ``hour``.
In order to calculate an average price.
Then match it with the minimum per group, that is given by ``weekday`` and ``hour``.
Counting the amount of occupying minima we obtain the following:
```{r warning=FALSE,message=FALSE}
df_comb_e10 <- df %>%
    group_by(nweek, weekday, hour) %>%
    summarise(e10=mean(e10)) %>%
    filter(e10 == min(e10)) %>%
    group_by(weekday, hour) %>%
    summarise(amount_e10 = n()) %>%
    arrange(desc(amount_e10))
```

```{r}
pander(head(df_comb_e10,10), style = 'rmarkdown')
```

Doing the same for **e5** and **diesel:
```{r warning=FALSE,message=FALSE}
df_comb_e5 <- df %>%
    group_by(nweek, weekday, hour) %>%
    summarise(e5=mean(e5)) %>%
    filter(e5 == min(e5)) %>%
    group_by(weekday, hour) %>%
    summarise(amount_e5 = n()) %>%
    arrange(desc(amount_e5))
```
```{r}
pander(head(df_comb_e5,10), style = 'rmarkdown')
```


```{r warning=FALSE,message=FALSE}
df_comb_diesel <- df %>%
    group_by(nweek, weekday, hour) %>%
    summarise(diesel=mean(diesel)) %>%
    filter(diesel == min(diesel)) %>%
    group_by(weekday, hour) %>%
    summarise(amount_diesel = n()) %>%
    arrange(desc(amount_diesel))
```
```{r}
pander(head(df_comb_diesel,10), style = 'rmarkdown')
```


Lastly we create a tabel, that contains the average prices for all weekdays at 7pm and 9pm.
```{r}
df %>%
      group_by(weekday, hour) %>%
      summarise(av_e10 = mean(e10),
                av_e5 = mean(e5),
                av_diesel = mean(diesel)) %>%
      filter(hour == 21 | hour == 19) %>%
      pander(style = 'rmarkdown')
```


# Where to Refuel?

## Brand Analysis

We start by calculating the brands with the highest amount of having the lowest price per day for e10.
Here is no consideration of how common the brand is or how many stations there are

```{r warning=FALSE,message=FALSE}
temp <- df %>%
  group_by(day,brand) %>%                         # group brand per day
  summarise(e10 = mean(e10)) %>%                  # calculate av price for each entry -> ungroups subgroup (brand)
  filter(e10 == min(e10)) %>%                     # find min for each group (day)
  group_by(brand) %>%                             # groups each brand over possible 366 days
  summarise(e10 = mean(e10), size=n()) %>%        # calculate av price per brand and count group size (how often was it min)
  arrange(desc(size))
```

Having a look at the output:
```{r}
pander(head(temp,10), style = 'rmarkdown')
```



In order to take the amount of stations by brand in consideration we consider ``stations.csv``.
```{r warning=FALSE,message=FALSE}
df_station <- read.csv("Datasets/stations.csv")

df_brand_size <- df_station %>%
  filter(brand!="") %>%           # rmoving unknown brands (blank)
  group_by(brand) %>%
  summarise(size=n()) %>%         # count how many stations are there (size of group)
  arrange(desc(size))
```


We will only consider brand with more than ``fsize`` stations.
In our case we set ``fsize = 300``.

```{r warning=FALSE,message=FALSE}
fsize <- 300

df_brand_size_fsize <- df_brand_size %>%
  filter(size >= fsize)
```

Repeating the evaluation from before, but only considering brand with 300 or more stations.
```{r warning=FALSE,message=FALSE}
temp <- df %>%
  filter(brand %in% df_brand_size_fsize$brand) %>%
  group_by(day,brand) %>%                         # group brand per day
  summarise(e10 = mean(e10))  %>%                 # calculate av price for each entry -> ungroups subgroup (brand)
  filter(e10 == min(e10)) %>%                     # find min for each group (day)
  group_by(brand) %>%                             # groups each brand over possible 366 days
  summarise(e10 = mean(e10), size=n()) %>%        # calculate av price per brand and count group size (how often was it min)
  arrange(desc(size))
```

Leads to the following:
```{r}
pander(head(temp,10), style = 'rmarkdown')
```


We see that **JET** is on **211** days the cheapest brand on average for e10, out of 366 possible days of 2024.

For **e5** and **diesel** we are following the example above:
```{r warning=FALSE,message=FALSE}
temp_e5 <- df %>%
  filter(brand %in% df_brand_size_fsize$brand) %>%
  group_by(day,brand) %>%                         # group brand per day
  summarise(e5 = mean(e5))  %>%                 # calculate av price for each entry -> ungroups subgroup (brand)
  filter(e5 == min(e5)) %>%                     # find min for each group (day)
  group_by(brand) %>%                             # groups each brand over possible 366 days
  summarise(e5 = mean(e5), size=n()) %>%        # calculate av price per brand and count group size (how often was it min)
  arrange(desc(size))

temp_diesel <- df %>%
  filter(brand %in% df_brand_size_fsize$brand) %>%
  group_by(day,brand) %>%                         # group brand per day
  summarise(diesel = mean(diesel))  %>%                 # calculate av price for each entry -> ungroups subgroup (brand)
  filter(diesel == min(diesel)) %>%                     # find min for each group (day)
  group_by(brand) %>%                             # groups each brand over possible 366 days
  summarise(diesel = mean(diesel), size=n()) %>%        # calculate av price per brand and count group size (how often was it min)
  arrange(desc(size))

```

Leads to the following:
```{r}
pander(head(temp_e5,10), style = 'rmarkdown')
```
```{r}
pander(temp_diesel, style = 'rmarkdown')
```








# Post Code Consideration
For this section, we will consider an example in order to reduce the need to work with a lot of the data.

## Transforming and Loading the Data
For this we follow the example of ``transforming_cleaning_agg_date_brand.R``, but instead of aggregating over all ids, we want to be able to differentiate the stations.
Since we would obtain a large and long file if we consider all the possible post codes, we only focus on one specific post code: **33100**.

```R
# joining/merging dataframes
library(dplyr)
# general handling analytics
library(tidyverse)


# open station file
df_station <- read.csv("Datasets/stations.csv")

# initiate an empty dataframe to build upon
df <-  data.frame()

# set post_code
postcode <-  33100

# filter df_station for post_code
df_station <- df_station %>%
            filter(post_code == postcode)


# getting a list of all files per month
# iterating over month
for (i in 1:12){
  # turn the int 3 into the str "03"
  if (i<10){
    folder <- paste0("0", toString(i))
  } else{
    folder <- toString(i)
  }
  path <- paste0("Datasets/2024/", folder)
  list_of_files <- list.files(path =path,
                            pattern = "\\.csv$",
                            full.names = TRUE)

  # iterating over each file (day) for the current month
  for (file_path in list_of_files){
    # open file (daily information)
    df_file <- read.csv(file_path) %>%
            inner_join(df_station, by =c("station_uuid"="uuid")) %>%
            select(date, station_uuid, post_code, city, name, brand, diesel,e5,e10) %>%
            filter(diesel>0.7,
                    diesel<3,
                    e5>0.7,
                    e5<3,
                    e10>0.7,
                    e10<3)   # We do that to avoid foulty outleyers

    # looking at the ``brand`` we notice some whitspace, let's remove it
    df_file$brand <- str_squish(df_file$brand)

    # set date_app YYYY-MM-DD HH
    df_file$date_app <- str_split_i(df_file$date, ":",1)
    temp <- df_file %>%
            group_by(date_app, station_uuid, post_code, city, name, brand) %>%
            summarise(diesel = mean(diesel),
                      e5 = mean(e5),
                      e10 = mean(e10),
                      gr_size = n())

    # add the day of the week as column
    temp$weekday <- weekdays(as.Date(str_split_i(temp$date_app, " ",1)))

    # add hour column
    temp$hour <- str_split_i(temp$date_app, " ",2)

    # add time of day colums that provides the information of time of day by
    # # night, morning, midday, evening in even 6h chuncks
    # # tod - time of day
    temp$tod <-  NA
    temp$tod[temp$hour %in% c("00", "01", "02", "03", "04", "05")] <- "night"
    temp$tod[temp$hour %in% c("06", "07", "08", "09", "10", "11")] <- "morning"
    temp$tod[temp$hour %in% c("12", "13", "14", "15", "16", "17")] <- "midday"
    temp$tod[temp$hour %in% c("18", "19", "20", "21", "22", "23")] <- "evening"

    # combine df with temp -- add rows from temp
    df <- rbind(df, temp)

    # remove ``temp`` and ``df_file`` from memory
    rm(temp,df_file)
  }
}

# adding month as column
df$month <- str_split_i(df$date_app, "-",2)

# adding calendar week as column
df$nweek <- strftime(str_split_i(df$date_app, " ",1), format = "%V")

# adding day as column
df$day <- str_split_i(df$date_app, " ",1)

#save file
write.csv(df,file='Datasets/agg_dataset_location.csv', row.names=FALSE)
```

## Where and When to Refuel?

We start by loading the prepared file.
```{r warning=FALSE,message=FALSE}
df_loc <- read.csv('Datasets/agg_dataset_location.csv')
```


We are interested in the best **weekday** and **location** to refuel.
Therefore, we will count minima occurrence per week, since the average does not provide a valuable result.
More precisely, we search the cheapest station per day and count their appearance.

```{r warning=FALSE,message=FALSE}
df_loc_day_diesel <- df_loc %>%
        group_by(nweek, weekday, station_uuid) %>%
        summarise(diesel=mean(diesel)) %>%
        filter(diesel == min(diesel)) %>%
        group_by(weekday, station_uuid) %>%
        summarise(size=n()) %>%
        inner_join(data.frame(df_station$uuid,df_station$name), by = c("station_uuid"="df_station.uuid")) %>% # for getting the station name
        arrange(weekday, desc(size))

df_loc_day_e5 <- df_loc %>%
        group_by(nweek, weekday, station_uuid) %>%
        summarise(e5=mean(e5)) %>%
        filter(e5 == min(e5)) %>%
        group_by(weekday, station_uuid) %>%
        summarise(size=n()) %>%
        inner_join(data.frame(df_station$uuid,df_station$name), by = c("station_uuid"="df_station.uuid")) %>% # for getting the station name
        arrange(weekday, desc(size))

df_loc_day_e10 <- df_loc %>%
        group_by(nweek, weekday, station_uuid) %>%
        summarise(e10=mean(e10)) %>%
        filter(e10 == min(e10)) %>%
        group_by(weekday, station_uuid) %>%
        summarise(size=n()) %>%
        inner_join(data.frame(df_station$uuid,df_station$name), by = c("station_uuid"="df_station.uuid")) %>% # for getting the station name
        arrange(weekday, desc(size))

# reorder columns:
df_loc_day_diesel <-  df_loc_day_diesel[,c(1,2,4,3)]
df_loc_day_e5 <-  df_loc_day_e5[,c(1,2,4,3)]
df_loc_day_e10 <-  df_loc_day_e10[,c(1,2,4,3)]
```


Thus, we obtain the following tables showing us where to refuel on each day of the week.
```{r}
pander(df_loc_day_diesel, style = 'rmarkdown', caption = 'diesel')
pander(df_loc_day_e5, style = 'rmarkdown', caption = 'e5')
pander(df_loc_day_e10, style = 'rmarkdown', caption = 'e10')
```

More precise, for each day of the week (``weekday``) we see how often which station was the cheapest throughout the year 2024.
For example, for **e10** on **Tuesdays** we see that **Tankstelle SB-Zentralmarkt** was on 33 out of 52 days the cheapest station that day.
So it would be not a bad idea to refuel there on Tuesdays.

Following the same ideas as before one can now continue to obtain the best station to refuel in the **evening** or at **11am** or even to refuel on **Mondays** at **3am**.

For this we will only focus on **e10**

### Overview for e10

#### Station per Day
```{r warning=FALSE,message=FALSE}
e10_day <- df_loc_day_e10 %>%
        filter(size==max(size))
```

```{r}
pander(e10_day, style = 'rmarkdown', caption = 'Best station for each day of the week -- e10 -- out of 52 day')
```


#### Station per Time of Day
```{r warning=FALSE,message=FALSE}
df_loc_tod_e10 <- df_loc %>%
        group_by(nweek, weekday, tod, station_uuid) %>%
        summarise(e10=mean(e10)) %>%
        filter(e10 == min(e10)) %>%
        group_by(tod, station_uuid) %>%
        summarise(size=n()) %>%
        inner_join(data.frame(df_station$uuid,df_station$name), by = c("station_uuid"="df_station.uuid")) %>% # for getting the station name
        arrange(tod, desc(size))

# reorder columns:
df_loc_tod_e10 <-  df_loc_tod_e10[,c(1,2,4,3)]

e10_tod <- df_loc_tod_e10 %>%
        filter(size==max(size))
```

```{r}
pander(e10_tod, style = 'rmarkdown', caption = 'Best station for each time of the day -- e10 -- out of 366 day')
```

#### Station per Time of Day
```{r warning=FALSE,message=FALSE}
df_loc_hour_e10 <- df_loc %>%
        group_by(nweek, weekday, hour, station_uuid) %>%
        summarise(e10=mean(e10)) %>%
        filter(e10 == min(e10)) %>%
        group_by(hour, station_uuid) %>%
        summarise(size=n()) %>%
        inner_join(data.frame(df_station$uuid,df_station$name), by = c("station_uuid"="df_station.uuid")) %>% # for getting the station name
        arrange(hour, desc(size))

# reorder columns:
df_loc_hour_e10 <-  df_loc_hour_e10[,c(1,2,4,3)]

e10_hour <- df_loc_hour_e10 %>%
        filter(size==max(size))
```

```{r}
pander(e10_hour, style = 'rmarkdown', caption = 'Best station for each hour of the day -- e10 -- out of 366 day')
```


